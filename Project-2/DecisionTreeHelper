{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRN2Wd0zw5dDv4NGWwkvLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"48EjyEEG9bVw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# to visualize data\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# to split data into training and test sets\n","from sklearn.model_selection import train_test_split\n","\n","# to build decision tree model\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","\n","# to tune different models\n","from sklearn.model_selection import GridSearchCV\n","\n","# To perform statistical analysis\n","import scipy.stats as stats\n","\n","# to compute classification metrics\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    f1_score,\n","    ConfusionMatrixDisplay,\n","    make_scorer,\n",")\n","# Library to suppress warnings or deprecation notes\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["def display_barplot(data, column, percentage=False, n=None):\n","    \"\"\"\n","    Barplot with percentage at the top\n","\n","    data: dataframe\n","    feature: dataframe column\n","    percentage: whether to display percentages instead of count (default is False)\n","    n: displays the top n category levels (default is None, i.e., display all levels)\n","    \"\"\"\n","\n","    total = len(data[column])  # length of the column\n","    count = data[column].nunique()\n","    if n is None:\n","        plt.figure(figsize=(count + 2, 6))\n","    else:\n","        plt.figure(figsize=(n + 2, 6))\n","\n","    plt.xticks(rotation=90, fontsize=15)\n","    ax = sns.countplot(\n","        data=data,\n","        x=column,\n","        palette=\"Paired\",\n","        order=data[column].value_counts().index[:n],\n","    )\n","\n","    for p in ax.patches:\n","        if percentage == True:\n","            label = \"{:.1f}%\".format(\n","                100 * p.get_height() / total\n","            )  # percentage of each class of the category\n","        else:\n","            label = p.get_height()  # count of each level of the category\n","\n","        x = p.get_x() + p.get_width() / 2  # width of the plot\n","        y = p.get_height()  # height of the plot\n","\n","        ax.annotate(\n","            label,\n","            (x, y),\n","            ha=\"center\",\n","            va=\"center\",\n","            size=12,\n","            xytext=(0, 5),\n","            textcoords=\"offset points\",\n","        )  # annotate the percentage\n","\n","    plt.show()  # show the plot"],"metadata":{"id":"8etaH-7SU7wH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to plot stacked bar chart\n","\n","\n","def display_stacked_barplot(data, column1, column2):\n","    \"\"\"\n","    Print the category counts and plot a stacked bar chart\n","\n","    data: dataframe\n","    predictor: independent variable\n","    target: target variable\n","    \"\"\"\n","    count = data[column1].nunique()\n","    sorter = data[column2].value_counts().index[-1]\n","    tab1 = pd.crosstab(data[column1], data[column2], margins=True).sort_values(\n","        by=sorter, ascending=False\n","    )\n","    print(tab1)\n","    print(\"-\" * 120)\n","    tab = pd.crosstab(data[column1], data[column2], normalize=\"index\").sort_values(\n","        by=sorter, ascending=False\n","    )\n","    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 6))\n","    plt.legend(\n","        loc=\"lower left\", frameon=False,\n","    )\n","    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n","    plt.show()"],"metadata":{"id":"FRi-ZJfPVCjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_effect_using_p_score(data, column, affected_column):\n","  crosstab = pd.crosstab(\n","  data[column], data[affected_column]\n","  )\n","\n","  Ho = column + \" has no effect on\" + affected_column  # Stating the none Hypothesis\n","  Ha = column + \" has an effect on\" + affected_column  # Stating the Alternate Hypothesis\n","\n","  chi, p_value, dof, expected = stats.chi2_contingency(crosstab)\n","\n","  if p_value < 0.05:  # Setting our significance level at 5%\n","      print(f\"{Ha} as the p_value ({p_value.round(3)}) < 0.05\")\n","  else:\n","      print(f\"{Ho} as the p_value ({p_value.round(3)}) > 0.05\")"],"metadata":{"id":"sK4V5GQmSM4_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_X_y(data, target_coll_name):\n","  X = data.drop(target_coll_name, axis=1)\n","  y = data[target_coll_name]\n","  return X, y"],"metadata":{"id":"k_CMotnv-2mT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining a function to compute different metrics to check performance of a classification model built using sklearn\n","def get_model_performance_details(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check classification model performance\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    acc = accuracy_score(target, pred)  # to compute Accuracy\n","    recall = recall_score(target, pred)  # to compute Recall\n","    precision = precision_score(target, pred)  # to compute Precision\n","    f1 = f1_score(target, pred)  # to compute F1-score\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n","        index=[0],\n","    )\n","\n","    return df_perf"],"metadata":{"id":"OY0N8fwpBiCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_confusion_matrix(model, predictors, target):\n","    \"\"\"\n","    To plot the confusion_matrix with percentages\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","    # Predict the target values using the provided model and predictors\n","    y_pred = model.predict(predictors)\n","\n","    # Compute the confusion matrix comparing the true target values with the predicted values\n","    cm = confusion_matrix(target, y_pred)\n","\n","    # Create labels for each cell in the confusion matrix with both count and percentage\n","    labels = np.asarray(\n","        [\n","            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n","            for item in cm.flatten()\n","        ]\n","    ).reshape(2, 2)    # reshaping to a matrix\n","\n","    # Set the figure size for the plot\n","    plt.figure(figsize=(6, 4))\n","\n","    # Plot the confusion matrix as a heatmap with the labels\n","    sns.heatmap(cm, annot=labels, fmt=\"\")\n","\n","    # Add a label to the y-axis\n","    plt.ylabel(\"True label\")\n","\n","    # Add a label to the x-axis\n","    plt.xlabel(\"Predicted label\")"],"metadata":{"id":"Wt2V5z99BlSt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_decision_tree (dtree, X_coll):\n","\n","  # set the figure size for the plot\n","  plt.figure(figsize=(20, 20))\n","\n","  # plotting the decision tree\n","  out = tree.plot_tree(\n","      dtree,                         # decision tree classifier model\n","      feature_names=X_coll,    # list of feature names (columns) in the dataset\n","      filled=True,                    # fill the nodes with colors based on class\n","      fontsize=9,                     # font size for the node text\n","      node_ids=False,                 # do not show the ID of each node\n","      class_names=None,               # whether or not to display class names\n","  )\n","\n","  # add arrows to the decision tree splits if they are missing\n","  for o in out:\n","      arrow = o.arrow_patch\n","      if arrow is not None:\n","          arrow.set_edgecolor(\"black\")    # set arrow color to black\n","          arrow.set_linewidth(1)          # set arrow linewidth to 1\n","\n","  # displaying the plot\n","  plt.show()"],"metadata":{"id":"oFESu0H2EQnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_decision_tree (dtree, X_coll):\n","  # printing a text report showing the rules of a decision tree\n","  print(\n","      tree.export_text(\n","          dtree,    # specify the model\n","          feature_names=X_coll,    # specify the feature names\n","          show_weights=True    # specify whether or not to show the weights associated with the model\n","      )\n","  )"],"metadata":{"id":"ator-VEQF3cC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_best_decision_tree_by_pre_pruning (X_train, y_train, X_test, y_test, max_depth_values, max_leaf_nodes_values, min_samples_split_values):\n","# initialize variables to store the best model and its performance\n","  best_estimator = None\n","  best_score_diff = float('inf')\n","\n","  # iterate over all combinations of the specified parameter values\n","  for max_depth in max_depth_values:\n","      for max_leaf_nodes in max_leaf_nodes_values:\n","          for min_samples_split in min_samples_split_values:\n","\n","              # initialize the tree with the current set of parameters\n","              estimator = DecisionTreeClassifier(\n","                  max_depth=max_depth,\n","                  max_leaf_nodes=max_leaf_nodes,\n","                  min_samples_split=min_samples_split,\n","                  random_state=42\n","              )\n","\n","              # fit the model to the training data\n","              estimator.fit(X_train, y_train)\n","\n","              # make predictions on the training and test sets\n","              y_train_pred = estimator.predict(X_train)\n","              y_test_pred = estimator.predict(X_test)\n","\n","              # calculate F1 scores for training and test sets\n","              train_f1_score = f1_score(y_train, y_train_pred)\n","              test_f1_score = f1_score(y_test, y_test_pred)\n","\n","              # calculate the absolute difference between training and test F1 scores\n","              score_diff = abs(train_f1_score - test_f1_score)\n","\n","              # update the best estimator and best score if the current one has a smaller score difference\n","              if score_diff < best_score_diff:\n","                  best_score_diff = score_diff\n","                  best_estimator = estimator\n","  return best_score_diff, best_estimator"],"metadata":{"id":"bp75VpITC_CN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_decision_tree_alpha_plots (X_train, y_train):\n","  # Create an instance of the decision tree model\n","  clf = DecisionTreeClassifier(random_state=42)\n","\n","  # Compute the cost complexity pruning path for the model using the training data\n","  path = clf.cost_complexity_pruning_path(X_train, y_train)\n","\n","  # Extract the array of effective alphas from the pruning path\n","  ccp_alphas = abs(path.ccp_alphas)\n","\n","  # Extract the array of total impurities at each alpha along the pruning path\n","  impurities = path.impurities\n","  print(pd.DataFrame(path))\n","\n","  # Create a figure\n","  fig, ax = plt.subplots(figsize=(10, 5))\n","\n","  # Plot the total impurities versus effective alphas, excluding the last value,\n","  # using markers at each data point and connecting them with steps\n","  ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n","\n","  # Set the x-axis label\n","  ax.set_xlabel(\"Effective Alpha\")\n","\n","  # Set the y-axis label\n","  ax.set_ylabel(\"Total impurity of leaves\")\n","\n","  # Set the title of the plot\n","  ax.set_title(\"Total Impurity vs Effective Alpha for training set\");\n","  plt.show()\n","\n","  # Initialize an empty list to store the decision tree classifiers\n","  clfs = []\n","\n","  # Iterate over each ccp_alpha value extracted from cost complexity pruning path\n","  for ccp_alpha in ccp_alphas:\n","      # Create an instance of the DecisionTreeClassifier\n","      clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)\n","\n","      # Fit the classifier to the training data\n","      clf.fit(X_train, y_train)\n","\n","      # Append the trained classifier to the list\n","      clfs.append(clf)\n","\n","  # Print the number of nodes in the last tree along with its ccp_alpha value\n","  print(\n","      \"Number of nodes in the last tree is {} with ccp_alpha {}\".format(\n","          clfs[-1].tree_.node_count, ccp_alphas[-1]\n","      )\n","  )\n","\n","  clfs = clfs[:-1]\n","  ccp_alphas = ccp_alphas[:-1]\n","\n","  # Extract the number of nodes in each tree classifier\n","  node_counts = [clf.tree_.node_count for clf in clfs]\n","\n","  # Extract the maximum depth of each tree classifier\n","  depth = [clf.tree_.max_depth for clf in clfs]\n","\n","  # Create a figure and a set of subplots\n","  fig, ax = plt.subplots(2, 1, figsize=(10, 7))\n","\n","  # Plot the number of nodes versus ccp_alphas on the first subplot\n","  ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n","  ax[0].set_xlabel(\"Alpha\")\n","  ax[0].set_ylabel(\"Number of nodes\")\n","  ax[0].set_title(\"Number of nodes vs Alpha\")\n","\n","  # Plot the depth of tree versus ccp_alphas on the second subplot\n","  ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n","  ax[1].set_xlabel(\"Alpha\")\n","  ax[1].set_ylabel(\"Depth of tree\")\n","  ax[1].set_title(\"Depth vs Alpha\")\n","\n","  # Adjust the layout of the subplots to avoid overlap\n","  fig.tight_layout()\n","  plt.show()\n","\n","  return ccp_alphas, clfs;"],"metadata":{"id":"LND4GEATIFpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_f1_scores(clfs, X, y):\n","  f1_scores = []  # Initialize an empty list to store F1 scores for training set for each decision tree classifier\n","  # Iterate through each decision tree classifier in 'clfs'\n","  for clf in clfs:\n","    # Predict labels for the training set using the current decision tree classifier\n","    pred = clf.predict(X)\n","\n","    # Calculate the F1 score for the training set predictions compared to true labels\n","    f1 = f1_score(y, pred)\n","\n","    # Append the calculated F1 score to the train_f1_scores list\n","    f1_scores.append(f1)\n","  return f1_scores;"],"metadata":{"id":"417HdKRBKaoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_f1_comparison(alphas, train_f1_scores, test_f1_scores):\n","  # Create a figure\n","  fig, ax = plt.subplots(figsize=(15, 5))\n","  ax.set_xlabel(\"Alpha\")  # Set the label for the x-axis\n","  ax.set_ylabel(\"F1 Score\")  # Set the label for the y-axis\n","  ax.set_title(\"F1 Score vs Alpha for training and test sets\")  # Set the title of the plot\n","\n","  # Plot the training F1 scores against alpha, using circles as markers and steps-post style\n","  ax.plot(alphas, train_f1_scores, marker=\"o\", label=\"training\", drawstyle=\"steps-post\")\n","\n","  # Plot the testing F1 scores against alpha, using circles as markers and steps-post style\n","  ax.plot(alphas, test_f1_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n","\n","  ax.legend()\n","  plt.show()"],"metadata":{"id":"RP09nrp8US5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_best_decision_tree_by_post_pruning(clfs, f1_scores):\n","  # creating the model where we get highest test F1 Score\n","  index_best_model = np.argmax(f1_scores)\n","\n","  # selcting the decision tree model corresponding to the highest test score\n","  best_decision_tree = clfs[index_best_model]\n","  print(best_decision_tree)\n","  return best_decision_tree"],"metadata":{"id":"XJFi6up9VtUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_decision_tree_comparison_metrics(dtree_train_perf, dtree_test_perf, name):\n","  models_train_comp_df = pd.concat(\n","      [\n","          dtree_train_perf.T,\n","          dtree_test_perf.T,\n","          dtree_train_perf.T - dtree_test_perf.T\n","      ],\n","      axis=1,\n","  )\n","  models_train_comp_df.columns = [\n","      \"(Train)\",\n","      \"(Test)\",\n","      \"(Diff)\"\n","  ]\n","  print(\"Performance comparison of:\", name)\n","  print(models_train_comp_df)"],"metadata":{"id":"CsvNSSuhXjIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_importance_of_features(decision_tree, feature_names):\n","  # importance of features in the tree building\n","  importances = decision_tree.feature_importances_\n","  indices = np.argsort(importances)\n","\n","  plt.figure(figsize=(10, 10))\n","  plt.title(\"Feature Importances\")\n","  plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n","  plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","  plt.xlabel(\"Relative Importance\")\n","  plt.show()\n"],"metadata":{"id":"W_8hxgHeYyQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_prediction_details(decision_tree, input_data):\n","  # making a prediction\n","  if (input_data.shape[0] > 1):\n","    raise Exception('Only one set of value allowed.')\n","  else:\n","    prediction = decision_tree.predict(input_data)\n","    print('Prediction:', prediction)\n","    prediction_likelihood = decision_tree.predict_proba(input_data)\n","    print('Likelyhood/Confidence:', prediction_likelihood[0, 1])"],"metadata":{"id":"ua5HrGfnaD8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_load():\n","  print(\"Decision Tree Helper Module Successfully Loaded\")"],"metadata":{"id":"A4EPEVuTeNrH"},"execution_count":null,"outputs":[]}]}